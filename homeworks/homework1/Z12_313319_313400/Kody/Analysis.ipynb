{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytywanie wszystkich plików CSV z wynikami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dla algorytmu random forest\n",
    "rf_rd_th  = pd.read_csv('../Wyniki/RandomForestClassifier/RandomSearch/allDatasets_tunability_history.csv')\n",
    "rf_rd_bm = pd.read_csv('../Wyniki/RandomForestClassifier/RandomSearch/b_m_tuning_history.csv')\n",
    "rf_rd_cd = pd.read_csv('../Wyniki/RandomForestClassifier/RandomSearch/c_d_tuning_history.csv')\n",
    "rf_rd_mt = pd.read_csv('../Wyniki/RandomForestClassifier/RandomSearch/m_t_tuning_history.csv')\n",
    "rf_rd_os = pd.read_csv('../Wyniki/RandomForestClassifier/RandomSearch/o_s_tuning_history.csv')\n",
    "rf_bo_th  = pd.read_csv('../Wyniki/RandomForestClassifier/BayesOptimization/allDatasets_tunability_history.csv')\n",
    "rf_bo_bm = pd.read_csv('../Wyniki/RandomForestClassifier/BayesOptimization/b_m_tuning_history.csv')\n",
    "rf_bo_cd = pd.read_csv('../Wyniki/RandomForestClassifier/BayesOptimization/c_d_tuning_history.csv')\n",
    "rf_bo_mt = pd.read_csv('../Wyniki/RandomForestClassifier/BayesOptimization/m_t_tuning_history.csv')\n",
    "rf_bo_os = pd.read_csv('../Wyniki/RandomForestClassifier/BayesOptimization/o_s_tuning_history.csv')\n",
    "rf_sc  = pd.read_csv('../Wyniki/RandomForestClassifier/sampling_comparison.csv')\n",
    "\n",
    "# Dla algorytmu gradient boosting\n",
    "gb_rd_th  = pd.read_csv('../Wyniki/GradientBoostingClassifier/RandomSearch/allDatasets_tunability_history.csv')\n",
    "gb_rd_bm = pd.read_csv('../Wyniki/GradientBoostingClassifier/RandomSearch/b_m_tuning_history.csv')\n",
    "gb_rd_cd = pd.read_csv('../Wyniki/GradientBoostingClassifier/RandomSearch/c_d_tuning_history.csv')\n",
    "gb_rd_mt = pd.read_csv('../Wyniki/GradientBoostingClassifier/RandomSearch/m_t_tuning_history.csv')\n",
    "gb_rd_os = pd.read_csv('../Wyniki/GradientBoostingClassifier/RandomSearch/o_s_tuning_history.csv')\n",
    "gb_bo_th  = pd.read_csv('../Wyniki/GradientBoostingClassifier/BayesOptimization/allDatasets_tunability_history.csv')\n",
    "gb_bo_bm = pd.read_csv('../Wyniki/GradientBoostingClassifier/BayesOptimization/b_m_tuning_history.csv')\n",
    "gb_bo_cd = pd.read_csv('../Wyniki/GradientBoostingClassifier/BayesOptimization/c_d_tuning_history.csv')\n",
    "gb_bo_mt = pd.read_csv('../Wyniki/GradientBoostingClassifier/BayesOptimization/m_t_tuning_history.csv')\n",
    "gb_bo_os = pd.read_csv('../Wyniki/GradientBoostingClassifier/BayesOptimization/o_s_tuning_history.csv')\n",
    "gb_sc  = pd.read_csv('../Wyniki/GradientBoostingClassifier/sampling_comparison.csv')\n",
    "\n",
    "# Dla algorytmu svm\n",
    "sv_rd_th  = pd.read_csv('../Wyniki/SVC/RandomSearch/allDatasets_tunability_history.csv')\n",
    "sv_rd_bm = pd.read_csv('../Wyniki/SVC/RandomSearch/b_m_tuning_history.csv')\n",
    "sv_rd_cd = pd.read_csv('../Wyniki/SVC/RandomSearch/c_d_tuning_history.csv')\n",
    "sv_rd_mt = pd.read_csv('../Wyniki/SVC/RandomSearch/m_t_tuning_history.csv')\n",
    "sv_rd_os = pd.read_csv('../Wyniki/SVC/RandomSearch/o_s_tuning_history.csv')\n",
    "sv_bo_th  = pd.read_csv('../Wyniki/SVC/BayesOptimization/allDatasets_tunability_history.csv')\n",
    "sv_bo_bm = pd.read_csv('../Wyniki/SVC/BayesOptimization/b_m_tuning_history.csv')\n",
    "sv_bo_cd = pd.read_csv('../Wyniki/SVC/BayesOptimization/c_d_tuning_history.csv')\n",
    "sv_bo_mt = pd.read_csv('../Wyniki/SVC/BayesOptimization/m_t_tuning_history.csv')\n",
    "sv_bo_os = pd.read_csv('../Wyniki/SVC/BayesOptimization/o_s_tuning_history.csv')\n",
    "sv_sc  = pd.read_csv('../Wyniki/SVC/sampling_comparison.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyświetlenie najlepszych hiperparametrów znalezionych daną metodą \n",
    "\n",
    "W wynikach widoczna jest także średnia i odchylenie standardowe uzyskiwane w wyniku cross walidacji jaki uzyskał dany model, biorąc pod uwagę wszystkie datasety przy treningu oraz hiperparametry domyślne. Hiperparametry domyślne są zgodne z dokumentacją sklearn. Linki do niej znajdują w raporcie. Hiperparametry są średnio najlepsze uwzględniając wszystkie datasety.\n",
    "\n",
    "Dla metody Bayes Optimization dla Random Forest najlepsza średnia powtarzała się, ale smac szuka pierwszego najlepszego wystąpienia, dlatego poniżej w kodzie zwracamy pierwszy wiersz z najlepszymi hiperparametrami. Tych powtórzeń jest stosunkowo mało (mniej niż 6 przy rozpatrywanych 150 punktach) co można uznać za naturalne zbieganie do granicy, a dowodem na to jest fakt, że dla innych metod nie zaobserwowano takiego zjawiska. \n",
    "\n",
    "Dla Random Search taka sytuacja nie miała miejsca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def searchMaxRowByColumn(dataframe, columnName):\n",
    "    return dataframe[dataframe[columnName] == dataframe[columnName].max()].iloc[0]\n",
    "\n",
    "print(\"\\nDla RandomForestClassifier:\\n\")\n",
    "rf_rd_best = searchMaxRowByColumn(rf_rd_th,'mean_all_datasets_test_score')\n",
    "print(\"Optymalne hiperparametry dla randomSearch:\\n\", rf_rd_best)\n",
    "rf_bo_best = searchMaxRowByColumn(rf_bo_th,'mean_all_datasets_test_score')\n",
    "print(\"Optymalne hiperparametry dla bayes optimization:\\n\", rf_bo_best)\n",
    "print(\"defaultowe hiperparametry: n_estimators: 100, min_samples_split:2, min_samples_leaf:1, max_features: none, max_depth: none \\n\")\n",
    "\n",
    "print(\"\\nDla GradientBoostingClassifier:\\n\")\n",
    "gb_rd_best = searchMaxRowByColumn(gb_rd_th,'mean_all_datasets_test_score') \n",
    "print(\"Optymalne hiperparametry dla randomSearch:\\n\", gb_rd_best)\n",
    "gb_bo_best = searchMaxRowByColumn(gb_bo_th,'mean_all_datasets_test_score')\n",
    "print(\"Optymalne hiperparametry dla bayes optimization:\\n\", gb_bo_best)\n",
    "print(\"defaultowe hiperparametry: n_estimators: 100, min_samples_split:2, min_samples_leaf:1, max_depth:3, learning_rate:0.1 ,subsample:1 \\n\")\n",
    "\n",
    "print(\"\\nDla SVC:\\n\")\n",
    "sv_rd_best = searchMaxRowByColumn(sv_rd_th,'mean_all_datasets_test_score') \n",
    "print(\"Optymalne hiperparametry dla randomSearch:\\n\", sv_rd_best)\n",
    "sv_bo_best = searchMaxRowByColumn(sv_bo_th,'mean_all_datasets_test_score')\n",
    "print(\"Optymalne hiperparametry dla bayes optimization:\\n\", sv_bo_best)\n",
    "print(\"defaultowe hiperparametry: C:1.0, kernel:rbf, gamma:1 / (n_features * X.var()) \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyniki jakie uzyskał model ze znalezionymi hiperparametrami na danych testowych.\n",
    "\n",
    "Ukazanie średniego wyniku oraz odchylenie standardowe jaki uzyskał dany model biorąc pod uwagę wszystkie datasety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDla RandomForestClassifier:\\n\")\n",
    "\n",
    "print(rf_sc)\n",
    "print(f\"randomSearch mean: {rf_sc['randomSearch_Model_auc_test'].mean()}, std:  {rf_sc['randomSearch_Model_auc_test'].std()}\\n\")\n",
    "print(f\"bayesOptimization mean: {rf_sc['bayesOptimization_Model_auc_test'].mean()}, std:  {rf_sc['bayesOptimization_Model_auc_test'].std()}\\n\")\n",
    "print(f\"default mean: {rf_sc['default_Model_auc_test'].mean()}, std:  {rf_sc['default_Model_auc_test'].std()}\\n\")\n",
    "print(f\"średnia poprawa dla randomSearch względem default: {rf_sc['randomSearch_Model_auc_test'].mean() - rf_sc['default_Model_auc_test'].mean()}\\n\")\n",
    "print(f\"średnia poprawa dla bayesOptimization względem default: {rf_sc['bayesOptimization_Model_auc_test'].mean() - rf_sc['default_Model_auc_test'].mean()}\\n\")\n",
    "\n",
    "print(\"\\nDla GradientBoostingClassifier:\\n\")\n",
    "print(gb_sc)\n",
    "print(f\"randomSearch mean: {gb_sc['randomSearch_Model_auc_test'].mean()}, std:  {gb_sc['randomSearch_Model_auc_test'].std()}\\n\")\n",
    "print(f\"bayesOptimization mean: {gb_sc['bayesOptimization_Model_auc_test'].mean()}, std:  {gb_sc['bayesOptimization_Model_auc_test'].std()}\\n\")\n",
    "print(f\"default mean: {gb_sc['default_Model_auc_test'].mean()}, std:  {gb_sc['default_Model_auc_test'].std()}\\n\")\n",
    "print(f\"średnia poprawa dla randomSearch względem default: {gb_sc['randomSearch_Model_auc_test'].mean() - gb_sc['default_Model_auc_test'].mean()}\\n\")\n",
    "print(f\"średnia poprawa dla bayesOptimization względem default: {gb_sc['bayesOptimization_Model_auc_test'].mean() - gb_sc['default_Model_auc_test'].mean()}\\n\")\n",
    "\n",
    "print(\"\\nDla SVC:\\n\")\n",
    "print(sv_sc)\n",
    "print(f\"randomSearch mean: {sv_sc['randomSearch_Model_auc_test'].mean()}, std:  {sv_sc['randomSearch_Model_auc_test'].std()}\\n\")\n",
    "print(f\"bayesOptimization mean: {sv_sc['bayesOptimization_Model_auc_test'].mean()}, std:  {sv_sc['bayesOptimization_Model_auc_test'].std()}\\n\")\n",
    "print(f\"default mean: {sv_sc['default_Model_auc_test'].mean()}, std:  {sv_sc['default_Model_auc_test'].std()}\\n\")\n",
    "print(f\"średnia poprawa dla randomSearch względem default: {sv_sc['randomSearch_Model_auc_test'].mean() - sv_sc['default_Model_auc_test'].mean()}\\n\")\n",
    "print(f\"średnia poprawa dla bayesOptimization względem default: {sv_sc['bayesOptimization_Model_auc_test'].mean() - sv_sc['default_Model_auc_test'].mean()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyżej każda średnia poprawa może być określona jako tunowalność przeprowadzona na zbiorze testowym. Może być ona porównana z tunowalnością, której oczekujemy. Będzie ona obliczona poniżej. \n",
    "\n",
    "Na podstawie informacji dostępnych powyżej można stwierdzić, że największą poprawę zaobserwowano dla SVM a najmniejsza dla Gradient Boosting. Z kolei Gradient Boosting był średnio najlepszym modelem, bez procesu tunowania hiperparametrów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykresy poszczególnych wyników średnich od iteracji dla danego modelu oraz dla danej metody samplingu\n",
    "\n",
    "Można zauważyć, że wyniki dla metody Bayes Optimization od pewnego momentu zbiegają do pewnej wartości, co nie zachodzi dla Random Search. Jest to spodziewana zależność."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iterations(dataframe,title, fileName):\n",
    "    y_values = dataframe['mean_all_datasets_test_score']\n",
    "    plt.plot(range(1, len(dataframe) + 1), y_values, marker='o', linestyle='-')\n",
    "    plt.xlabel('Iteracje')\n",
    "    plt.ylabel('Średnio najlepszy wynik')\n",
    "    plt.title(fileName)\n",
    "    plt.savefig('../Wyniki/' + fileName + '.jpg', format='jpeg', dpi=300, bbox_inches='tight')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_iterations(rf_rd_th,\"Średnio najlepszy wynik jaki uzyskał model random forest classifier biorąc pod uwagę wszystkie datasety dla random search.\", \"rf_rd\")\n",
    "plot_iterations(rf_bo_th,\"Średnio najlepszy wynik jaki uzyskał model random forest classifier biorąc pod uwagę wszystkie datasety dla bayes optimization.\", \"rf_bo\")\n",
    "\n",
    "plot_iterations(gb_rd_th,\"Średnio najlepszy wynik jaki uzyskał model gradient boosting classifier biorąc pod uwagę wszystkie datasety dla random search.\", \"gb_rd\")\n",
    "plot_iterations(gb_bo_th,\"Średnio najlepszy wynik jaki uzyskał model gradient boosting classifier biorąc pod uwagę wszystkie datasety dla bayes optimization.\", \"gb_bo\")\n",
    "\n",
    "plot_iterations(sv_rd_th,\"Średnio najlepszy wynik jaki uzyskał model svm classifier biorąc pod uwagę wszystkie datasety dla random search.\", \"sv_rd\")\n",
    "plot_iterations(sv_bo_th,\"Średnio najlepszy wynik jaki uzyskał model svm classifier biorąc pod uwagę wszystkie datasety dla bayes optimization.\", \"sv_bo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykresy najlepszych do tej pory znalezionych wyników średnich od iteracji dla danego modelu oraz dla danej metody samplingu\n",
    "\n",
    "Podczas przeprowadzania eksperymentu ustalono większą liczbę iteracji dla Random Search itencjonalnie, by zaprezentować, że liczba iteracji nie ma bezpośredniego wpływu na znaleziony najlepszy punkt w hiperprzestrzeni (oprócz oczywistego faktu, że czym więcej punktów tym większa szansza, że znajdziemy optimum).\n",
    "\n",
    "W Bayes Optimization widać natomiast, że potrzeba stosunkowo mało iteracji żeby osiągnąć stabilność. Widać to przykładowo dla wykresu o tytule \"Średnio najlepszy wynik jaki uzyskał model random forest classifier biorąc pod uwagę wszystkie datasety dla bayes optimization.\", gdzie najlepszy wynik uzyskano już przy 50 iteracji. Wykresy nie są najładniejsze, ale prezentując je chcieliśmy skupić się na zaprezentowaniu momentów, kiedy zostały znalezione nowe najlepsze hiperparametry, sam proces ich znajdowania widać lepiej na powyższych wykresach. Iterację, od której algorytmy są stabilne bez problemu można odczytać z wykresów. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iterations_accumulate(dataframe,title,fileName):\n",
    "    y_values = dataframe['mean_all_datasets_test_score']\n",
    "    plt.plot(range(1, len(dataframe) + 1), np.maximum.accumulate(y_values),  linestyle='-')\n",
    "    plt.xlabel('Iteracje')\n",
    "    plt.ylabel('Średnio najlepszy wynik do danej iteracji')\n",
    "    plt.title(fileName)\n",
    "    plt.savefig('../Wyniki/' + fileName + '.jpg', format='jpeg', dpi=300, bbox_inches='tight')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_iterations_accumulate(rf_rd_th,\"Średnio najlepszy wynik jaki uzyskał model random forest classifier biorąc pod uwagę wszystkie datasety dla random search.\", \"rf_rd_accumulate\")\n",
    "plot_iterations_accumulate(rf_bo_th,\"Średnio najlepszy wynik jaki uzyskał model random forest classifier biorąc pod uwagę wszystkie datasety dla bayes optimization.\", \"rf_bo_accumulate\")\n",
    "\n",
    "plot_iterations_accumulate(gb_rd_th,\"Średnio najlepszy wynik jaki uzyskał model gradient boosting classifier biorąc pod uwagę wszystkie datasety dla random search.\", \"gb_rd_accumulate\")\n",
    "plot_iterations_accumulate(gb_bo_th,\"Średnio najlepszy wynik jaki uzyskał model gradient boosting classifier biorąc pod uwagę wszystkie datasety dla bayes optimization.\", \"gb_bo_accumulate\")\n",
    "\n",
    "plot_iterations_accumulate(sv_rd_th,\"Średnio najlepszy wynik jaki uzyskał model svm classifier biorąc pod uwagę wszystkie datasety dla random search.\", \"sv_rd_accumulate\")\n",
    "plot_iterations_accumulate(sv_bo_th,\"Średnio najlepszy wynik jaki uzyskał model svm classifier biorąc pod uwagę wszystkie datasety dla bayes optimization.\", \"sv_bo_accumulate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunowalność"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zgodnie z artykułem Tunability: Importance of Hyperparameters of Machine Learning Algorithms, poniżej obliczamy tunowalność algorytmów.\n",
    "\n",
    "Wprowadzone oznaczenia: \n",
    "rf_rd_bm['mean_test_score'][rf_rd_best.name] - średnio najlepszy punkt w przestrzeni hiperparametrów dla wszystkich zbiorów danych dla danego algorytmu. \n",
    "\n",
    "O_{algorytm nazwa}_{metoda samplingu}_{i-ty zbiór danych} - najlepszy punkt w przestrzeni hiperparametrów dla danego algorytmu dla danego zbioru danych.  \n",
    "\n",
    "Poniżej będziemy posługiwać się wynikami uzyskanymi z cross walidacji, co może przewidywać o ile jesteśmy w stanie poprawić model na danych testowych. Z uwagi na to, że mamy policzone już jaką poprawę uzyskaliśmy na danych testowych będzie można to porównać."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policzmy najpierw O_{algorytm nazwa}_{metoda samplingu} - O_{algorytm nazwa}_{metoda samplingu}_{i-ty zbiór danych}\n",
    "\n",
    "O_rf_rd_bm = searchMaxRowByColumn(rf_rd_bm,'mean_test_score')['mean_test_score']\n",
    "O_rf_rd_cd = searchMaxRowByColumn(rf_rd_cd,'mean_test_score')['mean_test_score']\n",
    "O_rf_rd_mt = searchMaxRowByColumn(rf_rd_mt,'mean_test_score')['mean_test_score']\n",
    "O_rf_rd_os = searchMaxRowByColumn(rf_rd_os,'mean_test_score')['mean_test_score']\n",
    "O_rf_bo_bm = searchMaxRowByColumn(rf_bo_bm,'mean_test_score')['mean_test_score']\n",
    "O_rf_bo_cd = searchMaxRowByColumn(rf_bo_cd,'mean_test_score')['mean_test_score']\n",
    "O_rf_bo_mt = searchMaxRowByColumn(rf_bo_mt,'mean_test_score')['mean_test_score']\n",
    "O_rf_bo_os = searchMaxRowByColumn(rf_bo_os,'mean_test_score')['mean_test_score']\n",
    "\n",
    "O_gb_rd_bm = searchMaxRowByColumn(gb_rd_bm,'mean_test_score')['mean_test_score']\n",
    "O_gb_rd_cd = searchMaxRowByColumn(gb_rd_cd,'mean_test_score')['mean_test_score']\n",
    "O_gb_rd_mt = searchMaxRowByColumn(gb_rd_mt,'mean_test_score')['mean_test_score']\n",
    "O_gb_rd_os = searchMaxRowByColumn(gb_rd_os,'mean_test_score')['mean_test_score']\n",
    "O_gb_bo_bm = searchMaxRowByColumn(gb_bo_bm,'mean_test_score')['mean_test_score']\n",
    "O_gb_bo_cd = searchMaxRowByColumn(gb_bo_cd,'mean_test_score')['mean_test_score']\n",
    "O_gb_bo_mt = searchMaxRowByColumn(gb_bo_mt,'mean_test_score')['mean_test_score']\n",
    "O_gb_bo_os = searchMaxRowByColumn(gb_bo_os,'mean_test_score')['mean_test_score']\n",
    "\n",
    "O_sv_rd_bm = searchMaxRowByColumn(sv_rd_bm,'mean_test_score')['mean_test_score']\n",
    "O_sv_rd_cd = searchMaxRowByColumn(sv_rd_cd,'mean_test_score')['mean_test_score']\n",
    "O_sv_rd_mt = searchMaxRowByColumn(sv_rd_mt,'mean_test_score')['mean_test_score']\n",
    "O_sv_rd_os = searchMaxRowByColumn(sv_rd_os,'mean_test_score')['mean_test_score']\n",
    "O_sv_bo_bm = searchMaxRowByColumn(sv_bo_bm,'mean_test_score')['mean_test_score']\n",
    "O_sv_bo_cd = searchMaxRowByColumn(sv_bo_cd,'mean_test_score')['mean_test_score']\n",
    "O_sv_bo_mt = searchMaxRowByColumn(sv_bo_mt,'mean_test_score')['mean_test_score']\n",
    "O_sv_bo_os = searchMaxRowByColumn(sv_bo_os,'mean_test_score')['mean_test_score']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretacja wyników:\n",
    " \n",
    "Wyniku dodatniego dla tej różnicy nigdy nie uzyskamy, gdyż np O_rf_rd_bm będzie zawsze większe bądź równe rf_rd_bm['mean_test_score'][rf_rd_best.name]. \n",
    "\n",
    "Wynik ujmeny zmiennej d_rf_rd_os_best oznacza, że istnieje taki punkt w przestrzeni hiperprzestrzeni dla danego datasetu, który przy wyborze jego i założeniu, że inne wyniki dla innych datasetów nie zmienią się, wpłynąłby na poprawę średnio najlepszego wyniku. Oznacza to, że jeżeli byłoby spełnione założenie to jesteśmy w stanie znaleźć jeszcze lepszy hiperparametr w przestrzeni hiperparametrów generalizujący model jeszcze bardziej.  \n",
    "\n",
    "Ciekawsza wydaje nam się być suma d_rf_rd_bm + d_rf_rd_cd + d_rf_rd_mt + d_rf_rd_os, która mówi o tym jak bardzo daleko jesteśmy od optymalnego rozwiązania mając obecnie dostępną wiedzę o najlepszych hiperparametrach dla danego datasetu przy założeniu, że jesteśmy znaleźć taki hiperparametr, który będzie średnia tych najlepszych wyników. \n",
    "\n",
    "Możemy interpretować uzyskany wynik jako liczb punktów procentowych o jaką da się poprawić maksymalnie dany model. Im liczba jest bliżej zera, tym możemy uznać, że jesteśmy bliżej rozwiązania optymalnego dla danych datasetów. Aby zobaczyć średnią możliwą poprawę dla danego datasetu powinno się podzielić tą sumę przez liczbę datasetów.\n",
    "\n",
    "Można zobaczyć, że wyniki dla RandomForestClassifier oraz dla GradientBoostingClassifier są dość podobne niezależnie od metody samplingu, a wyniki dla SVM Classifier bardzo się różnią w stosunku do Bayes Optimization. Jest to według nas powód tego, że jest to algorytm, który jest najbardziej zależny od danych wejściowych, co jest pewnego rodzaju przesłanką, że znaczenie optymalizacji hiperparametrów dla SVM jest bardzo duże, co wiąże się z największym rezultatem średniej poprawy algorytmu dla danych testowych niezależnie od metody samplingu.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDla RandomForestClassifier:\\n\")\n",
    "d_rf_rd_bm_best = rf_rd_bm['mean_test_score'][rf_rd_best.name] - O_rf_rd_bm   \n",
    "d_rf_rd_cd_best = rf_rd_cd['mean_test_score'][rf_rd_best.name] - O_rf_rd_cd\n",
    "d_rf_rd_mt_best = rf_rd_mt['mean_test_score'][rf_rd_best.name] - O_rf_rd_mt\n",
    "d_rf_rd_os_best = rf_rd_os['mean_test_score'][rf_rd_best.name] - O_rf_rd_os\n",
    "d_rf_bo_bm_best = rf_bo_bm['mean_test_score'][rf_bo_best.name] - O_rf_bo_bm\n",
    "d_rf_bo_cd_best = rf_bo_cd['mean_test_score'][rf_bo_best.name] - O_rf_bo_cd\n",
    "d_rf_bo_mt_best = rf_bo_mt['mean_test_score'][rf_bo_best.name] - O_rf_bo_mt\n",
    "d_rf_bo_os_best = rf_bo_os['mean_test_score'][rf_bo_best.name] - O_rf_bo_os\n",
    "print(f'd_rf_rd_bm_best: {d_rf_rd_bm_best}\\n')\n",
    "print(f'd_rf_rd_cd_best: {d_rf_rd_cd_best}\\n')\n",
    "print(f'd_rf_rd_mt_best: {d_rf_rd_mt_best}\\n')\n",
    "print(f'd_rf_rd_os_best: {d_rf_rd_os_best}\\n')\n",
    "print(f'd_rf_bo_bm_best: {d_rf_bo_bm_best}\\n')\n",
    "print(f'd_rf_bo_cd_best: {d_rf_bo_cd_best}\\n')\n",
    "print(f'd_rf_bo_mt_best: {d_rf_bo_mt_best}\\n')\n",
    "print(f'd_rf_bo_os_best: {d_rf_bo_os_best}\\n')\n",
    "print(f'suma różnic dla random search: {d_rf_rd_bm_best+d_rf_rd_cd_best+d_rf_rd_mt_best+d_rf_rd_os_best}')\n",
    "print(f'suma różnic dla bayes optimization: {d_rf_bo_bm_best+d_rf_bo_cd_best+d_rf_bo_mt_best+d_rf_bo_os_best}')\n",
    "\n",
    "print(\"\\nDla GradientBoostingClassifier:\\n\")\n",
    "d_gb_rd_bm_best = gb_rd_bm['mean_test_score'][gb_rd_best.name] - O_gb_rd_bm\n",
    "d_gb_rd_cd_best = gb_rd_cd['mean_test_score'][gb_rd_best.name] - O_gb_rd_cd\n",
    "d_gb_rd_mt_best = gb_rd_mt['mean_test_score'][gb_rd_best.name] - O_gb_rd_mt\n",
    "d_gb_rd_os_best = gb_rd_os['mean_test_score'][gb_rd_best.name] - O_gb_rd_os\n",
    "d_gb_bo_bm_best = gb_bo_bm['mean_test_score'][gb_bo_best.name] - O_gb_bo_bm\n",
    "d_gb_bo_cd_best = gb_bo_cd['mean_test_score'][gb_bo_best.name] - O_gb_bo_cd\n",
    "d_gb_bo_mt_best = gb_bo_mt['mean_test_score'][gb_bo_best.name] - O_gb_bo_mt\n",
    "d_gb_bo_os_best = gb_bo_os['mean_test_score'][gb_bo_best.name] - O_gb_bo_os\n",
    "print(f'd_gb_rd_bm_best: {d_gb_rd_bm_best}\\n')\n",
    "print(f'd_gb_rd_cd_best: {d_gb_rd_cd_best}\\n')\n",
    "print(f'd_gb_rd_mt_best: {d_gb_rd_mt_best}\\n')\n",
    "print(f'd_gb_rd_os_best: {d_gb_rd_os_best}\\n')\n",
    "print(f'd_gb_bo_bm_best: {d_gb_bo_bm_best}\\n')\n",
    "print(f'd_gb_bo_cd_best: {d_gb_bo_cd_best}\\n')\n",
    "print(f'd_gb_bo_mt_best: {d_gb_bo_mt_best}\\n')\n",
    "print(f'd_gb_bo_os_best: {d_gb_bo_os_best}\\n')\n",
    "print(f'suma różnic dla random search: {d_gb_rd_bm_best+d_gb_rd_cd_best+d_gb_rd_mt_best+d_gb_rd_os_best}')\n",
    "print(f'suma różnic dla bayes optimization: {d_gb_bo_bm_best+d_gb_bo_cd_best+d_gb_bo_mt_best+d_gb_bo_os_best}')\n",
    "\n",
    "print(\"\\nDla SVC:\\n\")\n",
    "d_sv_rd_bm_best = sv_rd_bm['mean_test_score'][sv_rd_best.name] - O_sv_rd_bm\n",
    "d_sv_rd_cd_best = sv_rd_cd['mean_test_score'][sv_rd_best.name] - O_sv_rd_cd\n",
    "d_sv_rd_mt_best = sv_rd_mt['mean_test_score'][sv_rd_best.name] - O_sv_rd_mt\n",
    "d_sv_rd_os_best = sv_rd_os['mean_test_score'][sv_rd_best.name] - O_sv_rd_os\n",
    "d_sv_bo_bm_best = sv_bo_bm['mean_test_score'][sv_bo_best.name] - O_sv_bo_bm\n",
    "d_sv_bo_cd_best = sv_bo_cd['mean_test_score'][sv_bo_best.name] - O_sv_bo_cd\n",
    "d_sv_bo_mt_best = sv_bo_mt['mean_test_score'][sv_bo_best.name] - O_sv_bo_mt\n",
    "d_sv_bo_os_best = sv_bo_os['mean_test_score'][sv_bo_best.name] - O_sv_bo_os\n",
    "print(f'd_sv_rd_bm_best: {d_sv_rd_bm_best}\\n')\n",
    "print(f'd_sv_rd_cd_best: {d_sv_rd_cd_best}\\n')\n",
    "print(f'd_sv_rd_mt_best: {d_sv_rd_mt_best}\\n')\n",
    "print(f'd_sv_rd_os_best: {d_sv_rd_os_best}\\n')\n",
    "print(f'd_sv_bo_bm_best: {d_sv_bo_bm_best}\\n')\n",
    "print(f'd_sv_bo_cd_best: {d_sv_bo_cd_best}\\n')\n",
    "print(f'd_sv_bo_mt_best: {d_sv_bo_mt_best}\\n')\n",
    "print(f'd_sv_bo_os_best: {d_sv_bo_os_best}\\n')\n",
    "print(f'suma różnic dla random search: {d_sv_rd_bm_best+d_sv_rd_cd_best+d_sv_rd_mt_best+d_sv_rd_os_best}')\n",
    "print(f'suma różnic dla bayes optimization: {d_sv_bo_bm_best+d_sv_bo_cd_best+d_sv_bo_mt_best+d_sv_bo_os_best}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policzmy także powyższą różnice dla wszystkich wartości z hisotrii tunowalności, aby móc wykorzystać wszystkie dane, które udało się nam wygenerować i móc wyciągnąć jeszcze lepsze wnioski i umieścić to na boxplotach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_rf_rd_bm = rf_rd_bm['mean_test_score'][rf_rd_best.name] - rf_rd_bm['mean_test_score']\n",
    "d_rf_rd_cd = rf_rd_cd['mean_test_score'][rf_rd_best.name] - rf_rd_cd['mean_test_score'] \n",
    "d_rf_rd_mt = rf_rd_mt['mean_test_score'][rf_rd_best.name] - rf_rd_mt['mean_test_score'] \n",
    "d_rf_rd_os = rf_rd_os['mean_test_score'][rf_rd_best.name] - rf_rd_os['mean_test_score'] \n",
    "d_rf_bo_bm = rf_bo_bm['mean_test_score'][rf_bo_best.name] - rf_bo_bm['mean_test_score'] \n",
    "d_rf_bo_cd = rf_bo_cd['mean_test_score'][rf_bo_best.name] - rf_bo_cd['mean_test_score'] \n",
    "d_rf_bo_mt = rf_bo_mt['mean_test_score'][rf_bo_best.name] - rf_bo_mt['mean_test_score'] \n",
    "d_rf_bo_os = rf_bo_os['mean_test_score'][rf_bo_best.name] - rf_bo_os['mean_test_score'] \n",
    "\n",
    "d_gb_rd_bm = gb_rd_bm['mean_test_score'][gb_rd_best.name] - gb_rd_bm['mean_test_score'] \n",
    "d_gb_rd_cd = gb_rd_cd['mean_test_score'][gb_rd_best.name] - gb_rd_cd['mean_test_score'] \n",
    "d_gb_rd_mt = gb_rd_mt['mean_test_score'][gb_rd_best.name] - gb_rd_mt['mean_test_score'] \n",
    "d_gb_rd_os = gb_rd_os['mean_test_score'][gb_rd_best.name] - gb_rd_os['mean_test_score'] \n",
    "d_gb_bo_bm = gb_bo_bm['mean_test_score'][gb_bo_best.name] - gb_bo_bm['mean_test_score'] \n",
    "d_gb_bo_cd = gb_bo_cd['mean_test_score'][gb_bo_best.name] - gb_bo_cd['mean_test_score'] \n",
    "d_gb_bo_mt = gb_bo_mt['mean_test_score'][gb_bo_best.name] - gb_bo_mt['mean_test_score'] \n",
    "d_gb_bo_os = gb_bo_os['mean_test_score'][gb_bo_best.name] - gb_bo_os['mean_test_score'] \n",
    "\n",
    "d_sv_rd_bm = sv_rd_bm['mean_test_score'][sv_rd_best.name] - sv_rd_bm['mean_test_score'] \n",
    "d_sv_rd_cd = sv_rd_cd['mean_test_score'][sv_rd_best.name] - sv_rd_cd['mean_test_score'] \n",
    "d_sv_rd_mt = sv_rd_mt['mean_test_score'][sv_rd_best.name] - sv_rd_mt['mean_test_score'] \n",
    "d_sv_rd_os = sv_rd_os['mean_test_score'][sv_rd_best.name] - sv_rd_os['mean_test_score'] \n",
    "d_sv_bo_bm = sv_bo_bm['mean_test_score'][sv_bo_best.name] - sv_bo_bm['mean_test_score'] \n",
    "d_sv_bo_cd = sv_bo_cd['mean_test_score'][sv_bo_best.name] - sv_bo_cd['mean_test_score'] \n",
    "d_sv_bo_mt = sv_bo_mt['mean_test_score'][sv_bo_best.name] - sv_bo_mt['mean_test_score'] \n",
    "d_sv_bo_os = sv_bo_os['mean_test_score'][sv_bo_best.name] - sv_bo_os['mean_test_score'] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykresy tunowalności dla danego modelu oraz dla danej metody samplingu dla ustalonych zbiorów danych. Zieloną linią zaznaczone są wartości tunowalności dla najlepszego punktu w hiperprzestrzeni dla danego zbioru danych. Czerwoną linią zaznaczone są wartości tunowalności dla średnio najlepszego punktu w hiperprzestrzeni biorąc pod uwagę wszystkie zbiory danych. \n",
    "Pomarańczową linią zaznaczona jest mediana z wartości. Box: 25 - 75, Whis: 5 - 95."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wnioski z boxplotów\n",
    "\n",
    "1. Dla metody Random Search zawsze występuje więcej próbek odstających niż dla Bayes Optimization.\n",
    "2. Boxy są mniejsze dla Bayes Optimization, co ma związek ze zbieżnością algorytmu.\n",
    "3. Dla Bayes Optimization czerwona kreska prawie zawsze pokrywa się z pomarańczową, co wskazuję na stabilność i ma związek ze znalezionym ekstremum lokalnym. \n",
    "4. Oczekiwanym rezultatem jest zbliżanie się kreski czerwonej do kreski zielonej, czyli dążenie do 0 różnicy 2 komórki wyżej.\n",
    "5. Wszystkie wartości dodatnie można interpretować o ile model został poprawiony w skali roc_auc. \n",
    "6. Nie jest zaskoczeniem, że czerwona linia jest na wysokości zera, przez sposób kontrukcji różnic.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot(labels,series_data, indexOfBest,valuesOfBest, title, fileName):\n",
    "    lineMin = 1 - 0.22\n",
    "    lineMax = 1 + 0.22\n",
    "    fig, ax = plt.subplots()\n",
    "    boxplot = ax.boxplot(series_data, labels=labels, whis=[5, 95])\n",
    "    ax.set_ylabel('Tunowalność')\n",
    "    ax.set_xlabel('Zbiór danych')\n",
    "    ax.set_title(title)\n",
    "    for index, data,valueOfBestPerDataSet in zip(range(0, len(series_data)), series_data,valuesOfBest):    \n",
    "        ax.hlines(y=data[indexOfBest], xmin= index + lineMin, xmax= index +lineMax, color='red', linewidth=1)\n",
    "        ax.hlines(y=valueOfBestPerDataSet, xmin= index + lineMin, xmax= index +lineMax, color='green', linewidth=1)\n",
    "    plt.savefig('../Wyniki/' + fileName +'_boxplot'+ '.jpg', format='jpeg', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "labels = ['bm','cd','mt','os']\n",
    "\n",
    "title = \"Tunowalność Random Forest dla metody samplingu Random Search\"\n",
    "plot_boxplot(labels,[d_rf_rd_bm, d_rf_rd_cd, d_rf_rd_mt, d_rf_rd_os],rf_rd_best.name,[d_rf_rd_bm_best, d_rf_rd_cd_best, d_rf_rd_mt_best, d_rf_rd_os_best],title,\"rf_rd\")\n",
    "\n",
    "title = \"Tunowalność Random Forest dla metody samplingu Bayes Optimization\"\n",
    "plot_boxplot(labels,[d_rf_bo_bm, d_rf_bo_cd, d_rf_bo_mt, d_rf_bo_os],rf_bo_best.name,[d_rf_bo_bm_best, d_rf_bo_cd_best, d_rf_bo_mt_best, d_rf_bo_os_best],title,\"rf_bo\")\n",
    "\n",
    "title = \"Tunowalność Gradient Boosting dla metody samplingu Random Search\"\n",
    "plot_boxplot(labels,[d_gb_rd_bm, d_gb_rd_cd, d_gb_rd_mt, d_gb_rd_os],gb_rd_best.name,[d_gb_rd_bm_best, d_gb_rd_cd_best, d_gb_rd_mt_best, d_gb_rd_os_best],title,\"gb_rd\")\n",
    "\n",
    "title = \"Tunowalność Gradient Boosting dla metody samplingu Bayes Optimization\"\n",
    "plot_boxplot(labels,[d_gb_bo_bm, d_gb_bo_cd, d_gb_bo_mt, d_gb_bo_os],gb_bo_best.name,[d_gb_bo_bm_best, d_gb_bo_cd_best, d_gb_bo_mt_best, d_gb_bo_os_best],title,\"gb_bo\")\n",
    "\n",
    "title = \"Tunowalność SVM dla metody samplinu random search\"\n",
    "plot_boxplot(labels,[d_sv_rd_bm, d_sv_rd_cd, d_sv_rd_mt, d_sv_rd_os],sv_rd_best.name,[d_sv_rd_bm_best, d_sv_rd_cd_best, d_sv_rd_mt_best, d_sv_rd_os_best],title,\"sv_rd\")\n",
    "\n",
    "title = \"Tunowalność SVM dla metody samplinu bayes optimization\"\n",
    "plot_boxplot(labels,[d_sv_bo_bm, d_sv_bo_cd, d_sv_bo_mt, d_sv_bo_os],sv_bo_best.name,[d_sv_bo_bm_best, d_sv_bo_cd_best, d_sv_bo_mt_best, d_sv_bo_os_best],title,\"sv_bo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policzmy jeszcze średnią tunowalność dla każdego zbioru danych i uśrednijmy tunowalność wszystkich zbiorów danych (z uwzględnieniem metody samplingu).\n",
    "\n",
    "Zrobimy to dla wartości z powyższych boxplotów. Jest to spowodowane tym, że uważamy, że to wspólnie z wcześniejszymi wnioskami dostatecznie wskazuje znaczenie tunability dla poszczególnych algorytmów uczenia maszynowego. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretacja poniższych wyników: \n",
    "średnia tunowalność dla metody Random Search jest zawsze większa niż dla Bayes Optimization, ponieważ mamy bias sampling podczas metody Random Search i nie zawsze mamy reprezentatywne próbki. Bardziej wiarygodna wydaje się nam być tunowalność z Bayes Optimization, ponieważ średnia znajduję się bliżej mediany. \n",
    "\n",
    "W ogólności można stwierdzić, że najlepiej tunowalnym algorytmem jest SVM, a najgorzej tunowalnym Gradient Boosting, ale jest dokładnie odwrotnie jeżeli chodzi o średnią skuteczność modelu na zbiorze testowym. Zatem można stwierdzić, że czym większa skuteczność modelu podstawowego tym optymalizacja hiperparametrów daje mniej, ale nadal polepsza model (co wydaje się być naturalnym wnioskiem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMean(dataframe,name):\n",
    "    print(name + f': mean: {dataframe.mean()}\\n')\n",
    "\n",
    "\n",
    "printMean(d_rf_rd_bm,'d_rf_rd_bm')\n",
    "printMean(d_rf_rd_cd,'d_rf_rd_cd')\n",
    "printMean(d_rf_rd_mt,'d_rf_rd_mt')\n",
    "printMean(d_rf_rd_os,'d_rf_rd_os')    \n",
    "\n",
    "printMean((d_rf_rd_bm+d_rf_rd_cd+d_rf_rd_mt+d_rf_rd_os)/4,'d_rf_rd_all_datasets')\n",
    "\n",
    "printMean(d_rf_bo_bm,'d_rf_bo_bm')\n",
    "printMean(d_rf_bo_cd,'d_rf_bo_cd')\n",
    "printMean(d_rf_bo_mt,'d_rf_bo_mt')\n",
    "printMean(d_rf_bo_os,'d_rf_bo_os')\n",
    "\n",
    "printMean((d_rf_bo_bm+d_rf_bo_cd+d_rf_bo_mt+d_rf_bo_os)/4,'d_rf_bo_all_datasets')\n",
    "\n",
    "printMean(d_gb_rd_bm,'d_gb_rd_bm')\n",
    "printMean(d_gb_rd_cd,'d_gb_rd_cd')\n",
    "printMean(d_gb_rd_mt,'d_gb_rd_mt')\n",
    "printMean(d_gb_rd_os,'d_gb_rd_os')   \n",
    "\n",
    "printMean((d_gb_rd_bm+d_gb_rd_cd+d_gb_rd_mt+d_gb_rd_os)/4,'d_gb_rd_all_datasets')\n",
    "printMean(d_gb_bo_bm,'d_gb_bo_bm')\n",
    "printMean(d_gb_bo_cd,'d_gb_bo_cd')\n",
    "printMean(d_gb_bo_mt,'d_gb_bo_mt')\n",
    "printMean(d_gb_bo_os,'d_gb_bo_os')\n",
    "\n",
    "printMean((d_gb_bo_bm+d_gb_bo_cd+d_gb_bo_mt+d_gb_bo_os)/4,'d_gb_bo_all_datasets')\n",
    "\n",
    "printMean(d_sv_rd_bm,'d_sv_rd_bm')\n",
    "printMean(d_sv_rd_cd,'d_sv_rd_cd')\n",
    "printMean(d_sv_rd_mt,'d_sv_rd_mt')\n",
    "printMean(d_sv_rd_os,'d_sv_rd_os')    \n",
    "\n",
    "printMean((d_sv_rd_bm+d_sv_rd_cd+d_sv_rd_mt+d_sv_rd_os)/4,'d_sv_rd_all_datasets')\n",
    "\n",
    "printMean(d_sv_bo_bm,'d_sv_bo_bm')\n",
    "printMean(d_sv_bo_cd,'d_sv_bo_cd')\n",
    "printMean(d_sv_bo_mt,'d_sv_bo_mt')\n",
    "printMean(d_sv_bo_os,'d_sv_bo_os')\n",
    "\n",
    "printMean((d_sv_bo_bm+d_sv_bo_cd+d_sv_bo_mt+d_sv_bo_os)/4,'d_sv_bo_all_datasets')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
